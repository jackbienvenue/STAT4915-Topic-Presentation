---
title: "Processing and Visualizing Weather Data"
format: revealjs
revealjs-theme: "simple" 
revealjs-transition: "fade" 
highlight-style: "github"  
footer: "Jack Bienvenue | STAT4915" 
slide-number: true 
---

# Why Analyze Weather Data?

---

## Prediction & Prevention

:::: {style="display: flex;"}

:::: {.column1 style="padding-right: 180px;"}

Weather data analysis involves statistical data science skills:

- Big Data Cleaning and Processing
- Geospatial & Time Series Visualization

::::

:::: {.column2}

![](images/Santa_Ana_Wind.webp){ width=120% }

These contribute to resiliency and preparation for severe weather events!
::::

::::


---

## Disaster Support


Statistical Data Science & GIS techniques support:

- Grid Repair after events (ORNL)
- Optimized grid resiliency infrastructure upgrades 
- Climate change projections for storms
- Optimization of disaster relief resource allocations

<section class="centered-section">
  <img src="images/gridshock.webp" class="centered-image">
</section>

<style>
  .centered-section {
    display: flex;
    justify-content: center;
    align-items: center;
    height: 100vh; /* Optional: to center vertically on the whole page */
  }

  .centered-image {
    width: 30%;
    height: auto; /* Maintains aspect ratio */
  }
</style>



---

## Data Science Learning Priorities

<table>
  <tr>
    <td style="width: 33%;">Programming & Data Management</td>
    <td>Explore a new, challenging datatype, sharing a self-made resource</td>
  </tr>
  <tr>
    <td>Data Analysis</td>
    <td>Highlight use cases and UConn research objectives</td>
  </tr>
  <tr>
    <td>Data Visualization</td>
    <td>Share GIS tips and weather-specific visualization techniques</td>
  </tr>
  <tr>
    <td>Ethics of Data Collection</td>
    <td>Data is public and unbiased</td>
  </tr>
</table>

# Sourcing Weather Data

---

## European Center for Medium-range Weather Forecasts (ECMWF)

Widely used for academic studies in weather, hydrogology, and climatology research

More options:

- National Weather Service, 
- NASA, or 
- National Oceanic and Atmospheric Administration (NOAA).

## ECMWF in the News {.scrollable} 

![A Floridian news channel displaying the ECMWF & Global Forecast System's hurricane models during Hurricane Humberto, 2019. Source: *News 4 Jacksonville*](images/Hurricane_Models_Graphic.jpg)

<br><br>

<div style="text-align: center;">
  <img src="images/euro_model.gif" alt="ECMWF model gif during Hurricane Humberto" width="80%">
</div>


---

## A New File Type: "*.GRIB*"

The GRIB data type:

- Has existed in various iterations since 1985
- Built for regular updating & dissemination for weather data
- **GRI**dded **B**inary 
    - "Low storage"
    - Independent grid cell composition

---

## Obtaining *.GRIB* Files

These files are free and publicly available from ECMWF via the ERA5 Land Package, accessible at:

<br><br>

<div style="text-align: center;">
  <a href="https://www.ecmwf.int/en/era5-land" style="font-size: 52px; font-weight: bold; color: blue; text-decoration: underline;">
    https://www.ecmwf.int/en/era5-land
  </a>
</div>

---

## Preparing to Clean *.GRIB* Files

Peculiarities of *.GRIB* file processing:

- Published as single files for any number of grid cells for set time interval
- You may want the format as individual grid cells for a complete time series
- Import of *.GRIB* is very finnicky in Python

---

## Cleaning *.GRIB* Files

```{python}
#| eval: false
#| echo: true

# Package Import
import pandas as pd
import cfgrib
import os

def grib_folder_processing(earliest_file, latest_file, 
    input_directory, output_directory_path, hourly=False):

    ## FUNCTION PHASE 1: TAKING GRIB FILES FROM FOLDER, 
    ## CREATING ONE LARGE DF WITH ALL GRID CELL CENTROIDS

    # Extract year and month from the earliest and latest file 
    # names
    earliest_year = int(earliest_file.split('_')[-2])  
    # Extract year from filename (second-to-last part)
    earliest_month = int(earliest_file.split('_')[-1].split('.')[0])  
    # Extract month from filename (last part before extension)
    latest_year = int(latest_file.split('_')[-2])  
    # Extract year from filename (second-to-last part)
    latest_month = int(latest_file.split('_')[-1].split('.')[0])  
    # Extract month from filename (last part before extension)

    # Initialize an empty list to store DataFrames
    df_list = []

    # Loop through years and months
    current_year, current_month = earliest_year, earliest_month

    while (current_year < latest_year) or 
        (current_year == latest_year and current_month <= latest_month):

        # Construct the file name for the current year and month
        file_name = f"download_ERA5_LAND_package_" + \
            f"{current_year}_" + \
            f"{current_month:02d}.grib"
        file_path = os.path.join(input_directory, file_name)

        # Check if the file exists and is a GRIB file (not an index file)
        if os.path.exists(file_path) and file_name.endswith(".grib"):
            try:
                if not hourly: # Handles daily case

                    # Read the GRIB file and convert to DataFrame
                    hourly_data = cfgrib.open_dataset(
                        file_path,
                        backend_kwargs={
                            'filter_by_keys': {
                                'typeOfLevel': 'surface',
                                'step': 1  # 1 hour step size
                            }
                        }
                    ) # Attempt to resolve issue with steps
                    hourly_df = hourly_data.to_dataframe()
                    df_list.append(hourly_df)

                else:
                    for step in range(24):
                        hourly_data = cfgrib.open_dataset(
                            file_path,
                            backend_kwargs={
                                'filter_by_keys': {
                                    'typeOfLevel': 'surface',
                                    # Step for each clock hour(0 to 23)
                                    'step': step  
                                }
                            }
                        )
                        hourly_df = hourly_data.to_dataframe()

                        df_list.append(hourly_df)
    
                #####print(f"Successfully processed file: {file_name}")
            except Exception as e:
                print(f"Error processing file {file_name}: {e}")
        else:
            print(f"File {file_name} " + \
      "not found in directory or is not a valid GRIB file.")

        # Move to the next month
        current_month += 1
        if current_month > 12:  # Reset month, increment year
            current_month = 1
            current_year += 1

    # Check if any files were processed
    if len(df_list) == 0:
        raise ValueError("No valid GRIB files were processed." + \
            " Check the input directory and file names.")

    # Since latitude and longitude are being used as a part of a
    # multi-index with time, let's replicate the latitude and 
    # longitude columns so that they are still accessible 
    # after the concatenation

    # Solving the multi-index issue to assign new columns:
    for i, df in enumerate(df_list):
         # Reset index to move latitude and longitude to columns
        df = df.reset_index() 
        # Create latitude1 column
        df['latitude1'] = df['latitude'].round(5)  
        # Create longitude1 column
        df['longitude1'] = df['longitude'].round(5)  
        # Set index back to MultiIndex
        df.set_index(['time', 'latitude', 'longitude'], inplace=True)  
        
        # Reassign the modified DataFrame back to the list
        df_list[i] = df

    # Combine all DataFrames into one
    combined_df = pd.concat(df_list, ignore_index=True) 

    # Sort the DataFrame chronologically
    combined_df.sort_index(inplace=True)

    #-----------------------------------------------------------------------------------

    ## FUNCTION PHASE 2: COLLAPSING NEW LARGE DF INTO MANY 
    ## INDIVIDUAL GRID CELL CSVs

    '''
    In this section, we have a few steps:

    1. Isolate individual points throughout timeseries
    2. Sort them into their unique dataframes
    3. Go into each dataframe and make sure 
        the information is sorted chronologically
    4. Create new directory to store the new CSVs
    5. Export the CSVs into the new directory with appropriate names

    '''

    #1. Isolate individual points throughout timeseries

        #1.a Let's subset our variables so we only 
        #    take the ones we want:

    # List of variables you want to keep
    keep_variables = ['t2m', 'tp', 'u10', 'v10', 
        'latitude1', 'longitude1', 'valid_time']

    # Subset to drop extraneous columns
    df = combined_df[keep_variables] 


    #2. Sort individual points into their unique dataframes

        # Here, we'll use a group by clause to yield a list
        # of dataframes, each corresponding to an individual
        # point. This particular method preserves the lat/lon
        # combo as both the index for the new df and as 
        # accessible columns in the resulting dfs:

    grouped = [
    group.set_index(['latitude1', 'longitude1'])
    .reset_index()
    for _, group in df.groupby(['latitude1', 'longitude1'])
    ]


    #3. Go into each dataframe and make sure the information 
    #   is sorted chronologically

    for i, group_df in enumerate(grouped):
        # Convert 'valid_time' column to datetime with error handling
        group_df['valid_time'] = pd.to_datetime(group_df['valid_time'], 
                        format='%m/%d/%Y %I:%M:%S %p', errors='coerce')
        
    # Sort each df by 'valid_time'
    group_df = group_df.sort_values(by='valid_time')
    group_df = group_df.dropna


    #4. Create new directory to store the new CSVs

    #4.a Create nested function to make directory (if necessary,
    #               which in most cases should be) for CSV storage

    def create_directory(output_directory_path):
        if not os.path.exists(output_directory_path):
            os.makedirs(output_directory_path)

    #5. Export the CSVs into the new directory with appropriate names

        #5.a. Create function to name files appropriately 
        #     as they are being exported 

    def export_dfs_to_csv(dfs, output_directory_path): 
        create_directory(output_directory_path)  # Ensure directory exists

            # Iterate through the list of DataFrames
        for idx, group_df in enumerate(dfs):
            # Define a naming convention based on latitude and longitude
            # For example: "latitude_longitude_df_1.csv"

            # Get the first value 
            # (assuming all values are the same for each group)
            lat = group_df['latitude1'].iloc[0]  
            # Same as above for longitude
            lon = group_df['longitude1'].iloc[0]  

            lon = lon.round(5) # for consistency and brevity

            # Rename columns:
            group_df = group_df.rename(columns={'latitude1': 'latitude',
                              'longitude1': 'longitude', 
                              'valid_time': 'time'})

            group_df.dropna

            for i, group_df in enumerate(grouped):
                # Convert 'valid_time' column to datetime 
                # with error handling
                group_df['valid_time'] = pd.to_datetime(
                    group_df['valid_time'], 
                    format='%m/%d/%Y %I:%M:%S %p', errors='coerce'
                    )
                    
            # Sort each df by 'valid_time'
            group_df = group_df.sort_values(by='valid_time')
            
            # Create a filename using 
            # latitude, longitude, and an index
            filename = f"lat_{lat}_lon_{lon}_time_series_weather.csv"

            # Replace dots in latitude and longitude with underscores &
            # Split only on the last dot (the file extension)
            filename = filename.rsplit('.', 1)  
            filename[0] = filename[0].replace('.', '_') 

            # Join the filename back together
            filename = '.'.join(filename)
            
            # Construct the full path for the CSV file
            full_path = os.path.join(output_directory_path, filename)

            # Export the DataFrame to CSV
            group_df.to_csv(full_path, index=False)

        #5.b. Wrap it up by using the functions to export!

    export_dfs_to_csv(grouped, output_directory_path)
```

# Building Animated Weather Maps in Python

---

## GIS Skills Preface: Cartography

Most default Python visualizations do not include cartographic information:

:::: {style="display: flex;"}

:::: {.column1 style="padding-right: 180px;"}

- Title
- Legend
- Scale
- Metadata
    - Author
    - Date
    - Sources

::::

:::: {.column2}

![](images/good_cartography.webp)

::::

::::


---

## GIS Skills: Coordinate Projections

::::: {style="display: flex;"}

:::: {.column1}

::: {.section1}

**Geographic Coordinate Systems**

Describe points in terms of longitude and latitude, on the spherical surface of Earth

The most common datum is the 1984 World Geodetic System (WGS84).

The EPSG:4326 code is used as a python argument.

:::

::::

:::: {.column2}

::: {.section3}

**Projected Coordinate Systems**

Describe points in terms of cartesian coordinates. Must use tailor-fit PCS for your ROI.

:::

::: {.section4}

**This is section 4**

Hi

:::

::::

:::::

---

## GIS Visualization Platforms:


<table border="1" style="width: 100%; text-align: center; vertical-align: middle;">
  <tr>
    <th></th>
    <th>Python</th>
    <th>ArcGIS</th>
  </tr>
  <tr>
    <td style="vertical-align: middle;">Reproducibility</td>
    <td><img src="images/checkmark.png" alt="checkmark" style="width: 60px; height: 60px;"></td>
    <td></td>
  </tr>
  <tr>
    <td style="vertical-align: middle;">Learning GIS</td>
    <td></td>
    <td><img src="images/checkmark.png" alt="checkmark" style="width: 60px; height: 60px;"></td>
  </tr>
  <tr>
    <td style="vertical-align: middle;">WYSIWYG</td>
    <td></td>
    <td><img src="images/checkmark.png" alt="checkmark" style="width: 60px; height: 60px;"></td>
  </tr>
  <tr>
    <td style="vertical-align: middle;">Free</td>
    <td><img src="images/checkmark.png" alt="checkmark" style="width: 60px; height: 60px;"></td>
    <td></td>
  </tr>
</table>

---

## Mapping Package: 



---

# Scholarly References

# Image & Article Credits

https://gisgeography.com/state-plane-coordinate-system-spcs/

https://www.geographyrealm.com/whats-in-a-map/ 

https://www.latimes.com/california/story/2019-10-09/what-makes-the-santa-ana-winds-blow 

https://www.mprnews.org/story/2022/09/27/forecast-models-lock-in-on-ians-devastating-blow-to-florida - gif

